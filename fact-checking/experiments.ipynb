{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f58ae17-a71e-4c2c-be3b-f643321f720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset, load_from_disk, Dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6251a",
   "metadata": {},
   "source": [
    "Note: You have to request access to Llama-3-8B-Instruct and login to huggingface to run this notebook.\n",
    "\n",
    "- https://huggingface.co/docs/huggingface_hub/quick-start\n",
    "- https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d19c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, prompt, max_new_tokens=100):\n",
    "        torch.mps.empty_cache()\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\n",
    "            self.model.device\n",
    "        )\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                attention_mask=attention_mask,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "\n",
    "        output_text = self.tokenizer.decode(\n",
    "            output_ids[0][input_ids.shape[-1] :], skip_special_tokens=True\n",
    "        )\n",
    "        return output_text\n",
    "\n",
    "\n",
    "class Llama3_8b(Generator):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "        )\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "\n",
    "generator = Llama3_8b()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24037fac",
   "metadata": {},
   "source": [
    "<b style=\"color:red\">IMPORTANT</b>: Before running the experiments contained in this notebook, make sure that you have created the \"fever-fine-coarse\" dataset using the `create_fever_fine_coarse_dataset.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33890b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = load_from_disk(\"fever-fine-coarse\")\n",
    "print(f\"{len(claims)} claims loaded\")\n",
    "\n",
    "sample_size = 10\n",
    "print(f\"Sample {sample_size} claims for experiments\")\n",
    "claims_sample = claims.shuffle(seed=150620241351).select(range(sample_size))\n",
    "claims_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31905b54",
   "metadata": {},
   "source": [
    "# Quantitative Evaluation of Meta-Llama-3-8B-Instruct on FEVER-Fine-Coarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the results in the experiments\n",
    "\n",
    "def eval_accuracy(filename):\n",
    "    predictions = pd.read_csv(filename)\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    invalid = 0\n",
    "\n",
    "    supports_total = 0\n",
    "    supports_correct = 0\n",
    "\n",
    "    refutes_total = 0\n",
    "    refutes_correct = 0\n",
    "\n",
    "    nef_supports = 0\n",
    "    nef_refutes = 0\n",
    "\n",
    "    for _, item in predictions.iterrows():\n",
    "        label = item[\"label\"]\n",
    "        prediction = item[\"prediction\"]\n",
    "\n",
    "        if label == \"SUPPORTS\":\n",
    "            supports_total += 1\n",
    "            if label == prediction:\n",
    "                supports_correct += 1\n",
    "            elif prediction == \"NOT ENOUGH EVIDENCE\":\n",
    "                nef_supports += 1\n",
    "        elif label == \"REFUTES\":\n",
    "            refutes_total += 1\n",
    "            if label == prediction:\n",
    "                refutes_correct += 1\n",
    "            elif prediction == \"NOT ENOUGH EVIDENCE\":\n",
    "                nef_refutes += 1\n",
    "        \n",
    "        total += 1\n",
    "        if label == prediction:\n",
    "            correct += 1\n",
    "        elif prediction is None:\n",
    "            invalid += 1\n",
    "\n",
    "\n",
    "    print(f\"Total Accuracy: {correct}/{total} ({correct/total:.2f})\")\n",
    "    print(f\"Supports correct: {supports_correct}/{supports_total} ({supports_correct/supports_total:.2f})\")\n",
    "    print(f\"Refutes correct: {refutes_correct}/{refutes_total} ({refutes_correct/refutes_total:.2f})\")\n",
    "    print(f\"Not enough info (supports): {nef_supports}/{supports_total} ({nef_supports/supports_total:.2f})\")\n",
    "    print(f\"Not enough info (refutes): {nef_refutes}/{refutes_total} ({nef_refutes/refutes_total:.2f})\")\n",
    "    print(f\"Invalid predictions: {invalid} ({invalid/total:.2f})\")\n",
    "\n",
    "    return correct, supports_correct, supports_total, refutes_correct, refutes_total, nef_supports, nef_refutes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bee72d",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "\n",
    "Llama3-8b with gold evidence (fine) and 2 answer choices (SUPPORTS, REFUTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a53aa-57c6-49b1-ad3b-5b28fc1fcd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful, smart, kind, and efficient AI assistant who always fulfills the user requests to the best of its abilities and strictly sticks to the given instructions.\n",
    "\n",
    "Instructions:\n",
    "You answer SUPPORTS if context EXPLICITLY supports the claim.\n",
    "You answer REFUTES if the context EXPLICITLY refutes the claim.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Claim:\n",
    "{claim}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "set_seed(5243)\n",
    "\n",
    "invalid_count = 0\n",
    "predictions = []\n",
    "\n",
    "for claim in tqdm(claims_sample):\n",
    "    label = claim[\"label\"]\n",
    "    claim_text = claim[\"claim\"]\n",
    "    context = \"\\n\".join(claim[\"evidence_fine\"])\n",
    "    prompt = prompt_template.format(context=context, claim=claim_text)\n",
    "    response = generator(prompt, max_new_tokens=10)\n",
    "    \n",
    "    prediction = None\n",
    "    if \"SUPPORTS\" in response:\n",
    "        prediction = \"SUPPORTS\"\n",
    "    elif \"REFUTES\" in response:\n",
    "        prediction = \"REFUTES\"\n",
    "    \n",
    "    predictions.append({\"claim\": claim_text, \"context\": context, \"label\": label, \"prediction\": prediction})\n",
    "\n",
    "# Store\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.to_csv(\"predictions-gold-fine-2way.csv\", index=False)\n",
    "\n",
    "# Evaluate\n",
    "eval_accuracy(\"predictions-gold-fine-2way.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafe1eee",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "\n",
    "Llama3-8b with gold evidence (fine) and 3 answer choices (SUPPORTS, REFUTES, NOT ENOUGH INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff81c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful, smart, kind, and efficient AI assistant who always fulfills the user requests to the best of its abilities and strictly sticks to the given instructions.\n",
    "\n",
    "Instructions:\n",
    "You answer SUPPORTS if context EXPLICITLY supports the claim.\n",
    "You answer REFUTES if the context EXPLICITLY refutes the claim.\n",
    "You answer NOT ENOUGH EVIDENCE if the context does not provide enough information to explicitly support or refute the claim.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Claim:\n",
    "{claim}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "set_seed(5243)\n",
    "\n",
    "invalid_count = 0\n",
    "predictions = []\n",
    "for claim in tqdm(claims_sample):\n",
    "    label = claim[\"label\"]\n",
    "    claim_text = claim[\"claim\"]\n",
    "    context = \"\\n\".join(claim[\"evidence_fine\"])\n",
    "    prompt = prompt_template.format(context=context, claim=claim_text)\n",
    "    response = generator(prompt, max_new_tokens=10)\n",
    "    \n",
    "    prediction = None\n",
    "    if \"SUPPORTS\" in response:\n",
    "        prediction = \"SUPPORTS\"\n",
    "    elif \"REFUTES\" in response:\n",
    "        prediction = \"REFUTES\"\n",
    "    elif \"NOT ENOUGH EVIDENCE\" in response:\n",
    "        prediction = \"NOT ENOUGH EVIDENCE\"\n",
    "    \n",
    "    predictions.append({\"claim\": claim_text, \"context\": context, \"label\": label, \"prediction\": prediction})\n",
    "\n",
    "# Store\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.to_csv(\"predictions-gold-fine-3way.csv\", index=False)\n",
    "\n",
    "# Evaluate\n",
    "eval_accuracy(\"predictions-gold-fine-3way.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0a477e",
   "metadata": {},
   "source": [
    "# Experiment 3\n",
    "\n",
    "Llama3-8b with gold evidence (coarse) and 2 answer choices (SUPPORTS, REFUTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9469c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful, smart, kind, and efficient AI assistant who always fulfills the user requests to the best of its abilities and strictly sticks to the given instructions.\n",
    "\n",
    "Instructions:\n",
    "You answer SUPPORTS if context EXPLICITLY supports the claim.\n",
    "You answer REFUTES if the context EXPLICITLY refutes the claim.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Claim:\n",
    "{claim}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "set_seed(5243)\n",
    "\n",
    "invalid_count = 0\n",
    "predictions = []\n",
    "\n",
    "for claim in tqdm(claims_sample):\n",
    "    label = claim[\"label\"]\n",
    "    claim_text = claim[\"claim\"]\n",
    "    context = \"\\n\".join(claim[\"evidence_coarse\"])\n",
    "    prompt = prompt_template.format(context=context, claim=claim_text)\n",
    "    response = generator(prompt, max_new_tokens=10)\n",
    "    \n",
    "    prediction = None\n",
    "    if \"SUPPORTS\" in response:\n",
    "        prediction = \"SUPPORTS\"\n",
    "    elif \"REFUTES\" in response:\n",
    "        prediction = \"REFUTES\"\n",
    "    \n",
    "    predictions.append({\"claim\": claim_text, \"context\": context, \"label\": label, \"prediction\": prediction})\n",
    "\n",
    "# Store\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.to_csv(\"predictions-gold-coarse.csv\", index=False)\n",
    "\n",
    "# Evaluate\n",
    "eval_accuracy(\"predictions-gold-coarse.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea3e30",
   "metadata": {},
   "source": [
    "# Experiment 4\n",
    "\n",
    "Llama3-8b with gold evidence (coarse) and 3 answer choices (SUPPORTS, REFUTES, NOT ENOUGH INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f0524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful, smart, kind, and efficient AI assistant who always fulfills the user requests to the best of its abilities and strictly sticks to the given instructions.\n",
    "\n",
    "Instructions:\n",
    "You answer SUPPORTS if context EXPLICITLY supports the claim.\n",
    "You answer REFUTES if the context EXPLICITLY refutes the claim.\n",
    "You answer NOT ENOUGH EVIDENCE if the context does not provide enough information to explicitly support or refute the claim.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Claim:\n",
    "{claim}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "set_seed(5243)\n",
    "\n",
    "invalid_count = 0\n",
    "predictions = []\n",
    "\n",
    "for claim in tqdm(claims_sample):\n",
    "    label = claim[\"label\"]\n",
    "    claim_text = claim[\"claim\"]\n",
    "    context = \"\\n\".join(claim[\"evidence_coarse\"])\n",
    "    prompt = prompt_template.format(context=context, claim=claim_text)\n",
    "    response = generator(prompt, max_new_tokens=10)\n",
    "    \n",
    "    prediction = None\n",
    "    if \"SUPPORTS\" in response:\n",
    "        prediction = \"SUPPORTS\"\n",
    "    elif \"REFUTES\" in response:\n",
    "        prediction = \"REFUTES\"\n",
    "    elif \"NOT ENOUGH EVIDENCE\" in response:\n",
    "        prediction = \"NOT ENOUGH EVIDENCE\"\n",
    "    \n",
    "    predictions.append({\"claim\": claim_text, \"context\": context, \"label\": label, \"prediction\": prediction})\n",
    "\n",
    "# Store the results\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.to_csv(\"predictions-gold-coarse-3way.csv\", index=False)\n",
    "\n",
    "eval_accuracy(\"predictions-gold-coarse-3way.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27c36e",
   "metadata": {},
   "source": [
    "# Experiment 5\n",
    "\n",
    "Llama3-8b without context (parametric knowledge only) and 2 answer choices (SUPPORTS, REFUTES)\n",
    "\n",
    "Note: The fact verification task is to tell whether a claim is supported or refuted by the given evidence.\n",
    "Here, the model just needs to predict whether it \"thinks\" the claim is \"true\" or \"false\" based on the knowledge it has been trained on.\n",
    "Strictly speaking, this is not a fact verification task, but shall serve as a baseline for the fact verification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9e7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful, smart, kind, and efficient AI assistant who always fulfills the user requests to the best of its abilities and strictly sticks to the given instructions.\n",
    "\n",
    "Instructions:\n",
    "You answer SUPPORTS if the claim is true.\n",
    "You answer REFUTES if the claim is false.\n",
    "\n",
    "Claim:\n",
    "{claim}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "set_seed(5243)\n",
    "\n",
    "invalid_count = 0\n",
    "predictions = []\n",
    "\n",
    "for claim in tqdm(claims_sample):\n",
    "    label = claim[\"label\"]\n",
    "    claim_text = claim[\"claim\"]\n",
    "    prompt = prompt_template.format(claim=claim_text)\n",
    "    response = generator(prompt, max_new_tokens=10)\n",
    "\n",
    "    prediction = None\n",
    "    if \"SUPPORTS\" in response:\n",
    "        prediction = \"SUPPORTS\"\n",
    "    elif \"REFUTES\" in response:\n",
    "        prediction = \"REFUTES\"\n",
    "\n",
    "    predictions.append({\"claim\": claim_text, \"label\": label, \"prediction\": prediction})\n",
    "\n",
    "# Store the results\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.to_csv(\"predictions-parametric.csv\", index=False)\n",
    "\n",
    "eval_accuracy(\"predictions_parametric.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eca55d1",
   "metadata": {},
   "source": [
    "# Experiment 6\n",
    "\n",
    "Llama3-8b without context (parametric knowledge only) and 3 answer choices (SUPPORTS, REFUTES, NOT ENOUGH INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7338c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful, smart, kind, and efficient AI assistant who always fulfills the user requests to the best of its abilities and strictly sticks to the given instructions.\n",
    "\n",
    "Instructions:\n",
    "You answer SUPPORTS if the claim is true.\n",
    "You answer REFUTES if the claim is false.\n",
    "You answer NOT ENOUGH EVIDENCE if your are not sure.\n",
    "\n",
    "Claim:\n",
    "{claim}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "set_seed(5243)\n",
    "\n",
    "invalid_count = 0\n",
    "predictions = []\n",
    "\n",
    "for claim in tqdm(claims_sample):\n",
    "    label = claim[\"label\"]\n",
    "    claim_text = claim[\"claim\"]\n",
    "    prompt = prompt_template.format(claim=claim_text)\n",
    "    response = generator(prompt, max_new_tokens=10)\n",
    "\n",
    "    prediction = None\n",
    "    if \"SUPPORTS\" in response:\n",
    "        prediction = \"SUPPORTS\"\n",
    "    elif \"REFUTES\" in response:\n",
    "        prediction = \"REFUTES\"\n",
    "\n",
    "    predictions.append({\"claim\": claim_text, \"label\": label, \"prediction\": prediction})\n",
    "\n",
    "# Store the results\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.to_csv(\"predictions-parametric-3way.csv\", index=False)\n",
    "\n",
    "eval_accuracy(\"predictions-parametric-3way.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e499114",
   "metadata": {},
   "source": [
    "## Experiment 7\n",
    "\n",
    "Create the retriever and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"./chroma_facts2\")\n",
    "embed = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\").encode\n",
    "\n",
    "n = 500000\n",
    "try:\n",
    "    db = client.get_collection(f\"claims-n{n}\")\n",
    "except:\n",
    "    db = client.create_collection(\n",
    "        name=f\"claims-n{n}\"\n",
    "    )\n",
    "\n",
    "    wiki_pages = load_dataset(\"fever/fever\", \"wiki_pages\")[\"wikipedia_pages\"]\n",
    "    wiki_pages_sample = wiki_pages.shuffle(seed=42).select(range(n))\n",
    "\n",
    "    pages = set()\n",
    "    for page in tqdm(wiki_pages_sample):\n",
    "        pages.add(page[\"text\"])\n",
    "\n",
    "    claims = load_from_disk(\"fever_fine_coarse\")\n",
    "    for claim in tqdm(claims):\n",
    "        for txt in claim[\"evidence_coarse\"]:\n",
    "            pages.add(txt)\n",
    "        for txt in claim[\"evidence_fine\"]:\n",
    "            pages.add(txt)\n",
    "\n",
    "    pages = list(pages)\n",
    "\n",
    "    for i, txt in enumerate(tqdm(pages)):\n",
    "        documents = [txt]\n",
    "        embeddings = [embed(txt).tolist()]\n",
    "        db.add(ids=[str(i)], documents=documents, embeddings=embeddings)\n",
    "\n",
    "retriever = lambda query, k: db.query(\n",
    "    query_embeddings=embed(query).tolist(),\n",
    "    n_results=k,\n",
    ")[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5017cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    data = []\n",
    "    for claim in tqdm(claims_sample):\n",
    "        evidence_retriever = retriever(claim[\"claim\"], i)\n",
    "        evidence_gt = claim[\"evidence_fine\"] + claim[\"evidence_coarse\"]\n",
    "\n",
    "        evidence_found = False\n",
    "        for evidence in evidence_gt:\n",
    "            for evidence_ in evidence_retriever:\n",
    "                if evidence in evidence_:\n",
    "                    evidence_found = True\n",
    "\n",
    "        data.append(\n",
    "            {\n",
    "                \"claim\": claim[\"claim\"],\n",
    "                \"label\": claim[\"label\"],\n",
    "                \"evidence_found\": evidence_found,\n",
    "                \"evidence_gt\": evidence_gt,\n",
    "                \"evidence_retriever\": evidence_retriever,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    data_df = pd.DataFrame(data)\n",
    "    data_hf = Dataset.from_pandas(data_df)\n",
    "    data_hf.save_to_disk(f\"claims_sample_retrieval_2_top_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for k in range(1, 11):\n",
    "    data = load_from_disk(f\"claims_sample_retrieval_2_top_{i}\")\n",
    "\n",
    "    count = 0\n",
    "    for row in data:\n",
    "        count += 1 if row[\"evidence_found\"] else 0\n",
    "    \n",
    "    accuracy = count/len(data)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "ax = plt.plot(range(1, 11), accuracies, marker='o', label='With Evidence (Total)')\n",
    "plt.title(\"Top-k retrieval accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5400ea48",
   "metadata": {},
   "source": [
    "## Experiment 8\n",
    "\n",
    "Evaluate the retriever and the generator together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1389440",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful, smart, kind, and efficient AI assistant who always fulfills the user requests to the best of its abilities and strictly sticks to the given instructions.\n",
    "\n",
    "Instructions:\n",
    "You answer SUPPORTS if context EXPLICITLY supports the claim.\n",
    "You answer REFUTES if the context EXPLICITLY refutes the claim.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Claim:\n",
    "{claim}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "for k in [1, 2, 3, 4, 5, 6, 10]:\n",
    "    set_seed(5243)\n",
    "    data = load_from_disk(f\"claims_sample_retrieval_top_{k}\")\n",
    "\n",
    "    predictions = []\n",
    "    for row in tqdm(data):\n",
    "        label = row[\"label\"]\n",
    "        claim = row[\"claim\"]\n",
    "        context = \"\\n\".join(row[\"evidence_retriever\"])\n",
    "        prompt = prompt_template.format(context=context, claim=claim)\n",
    "        response = generator(prompt, max_new_tokens=10)\n",
    "        \n",
    "        prediction = None\n",
    "        if \"SUPPORTS\" in response:\n",
    "            prediction = \"SUPPORTS\"\n",
    "        elif \"REFUTES\" in response:\n",
    "            prediction = \"REFUTES\"\n",
    "        \n",
    "        predictions.append({\"claim\": claim, \"context\": context, \"label\": label, \"prediction\": prediction, \"evidence_found\": row[\"evidence_found\"]})\n",
    "\n",
    "    # stroe predictions\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    predictions_df.to_csv(f\"predictions_claims_sample_retrieval_top_{k}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b1668",
   "metadata": {},
   "source": [
    "Compute the average context length of $k$ retrieved documents for $k = 1, 2, 3, 4, 5, 6, 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc678fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "avg_context_len_list = []\n",
    "for k in [1, 2, 3, 4, 5, 6, 10]:\n",
    "    predictions = pd.read_csv(f\"predictions_claims_sample_retrieval_top_{k}.csv\")\n",
    "\n",
    "    context_len_avg = 0\n",
    "    for _, item in tqdm(predictions.iterrows()):\n",
    "        context = item[\"context\"]\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "        inputs = tokenizer(context, return_tensors=\"pt\")\n",
    "        context_len = len(inputs[\"input_ids\"][0])\n",
    "        context_len_avg += context_len\n",
    "\n",
    "    context_len_avg /= len(predictions)\n",
    "    avg_context_len_list.append(context_len_avg)\n",
    "print(avg_context_len_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81e1353",
   "metadata": {},
   "source": [
    "# Plots\n",
    "\n",
    "Note: To compute the plots you have to enter the data obtained from the evaluation function manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9058ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_accuracies = []\n",
    "supports_accuracies = []\n",
    "refutes_accuracies = []\n",
    "\n",
    "for k in [10]:\n",
    "    predictions = pd.read_csv(f\"predictions_claims_sample_retrieval_top_{k}.csv\")\n",
    "\n",
    "    label_supports = 0\n",
    "    label_refutes = 0\n",
    "    correct_supports = 0\n",
    "    correct_refutes = 0\n",
    "\n",
    "    for _, item in predictions.iterrows():\n",
    "        label = item[\"label\"]\n",
    "        prediction = item[\"prediction\"]\n",
    "        claim = item[\"claim\"]\n",
    "        context = item[\"context\"]\n",
    "        evidence_found = bool(item[\"evidence_found\"])\n",
    "\n",
    "        if not evidence_found:\n",
    "            continue\n",
    "\n",
    "        if label == \"SUPPORTS\":\n",
    "            label_supports += 1\n",
    "            if label == prediction:\n",
    "                correct_supports += 1\n",
    "        elif label == \"REFUTES\":\n",
    "            label_refutes += 1\n",
    "            if label == prediction:\n",
    "                correct_refutes += 1\n",
    "        else:\n",
    "            print(\"Unknown label...\")\n",
    "\n",
    "    total_accuracies.append((correct_supports + correct_refutes)/(label_supports + label_refutes))\n",
    "    supports_accuracies.append(correct_supports/label_supports)\n",
    "    refutes_accuracies.append(correct_refutes/label_refutes)\n",
    "\n",
    "print(f\"Total accuracies: {total_accuracies}\")\n",
    "print(f\"Supports accuracies: {supports_accuracies}\")\n",
    "print(f\"Refutes accuracies: {refutes_accuracies}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a362cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_context_length = [184.72, 334.745, 466.355, 608.745, 736.585, 1359.502512562814]\n",
    "total_accuracies_with_evidence = [0.9764705882352941, 0.970873786407767, 0.9629629629629629, 0.9724770642201835, 0.9553571428571429, 0.9473684210526315]\n",
    "supports_accuracies_with_evidence = [0.9622641509433962, 0.9682539682539683, 0.9545454545454546, 0.9696969696969697, 0.9558823529411765, 0.9285714285714286]\n",
    "refutes_accuracies_with_evidence = [1.0, 0.975, 0.9761904761904762, 0.9767441860465116, 0.9545454545454546, 0.9772727272727273]\n",
    "total_accuracies_without_evidence = [0.6782608695652174, 0.6494845360824743, 0.6521739130434783, 0.6703296703296703, 0.6363636363636364, 0.6235294117647059]\n",
    "supports_accuracies_without_evidence = [0.33962264150943394, 0.27906976744186046, 0.225, 0.3, 0.23684210526315788, 0.19444444444444445]\n",
    "refutes_accuracies_without_evidence = [0.967741935483871, 0.9444444444444444, 0.9807692307692307, 0.9607843137254902, 0.94, 0.9387755102040817]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.plot(avg_context_length, total_accuracies_with_evidence, marker='o', label='With Evidence (Total)')\n",
    "plt.plot(avg_context_length, supports_accuracies_with_evidence, marker='o', label='With Evidence (Supports)')\n",
    "plt.plot(avg_context_length, refutes_accuracies_with_evidence, marker='o', label='With Evidence (Refutes)')\n",
    "plt.plot(avg_context_length, total_accuracies_without_evidence, linestyle='dashed', marker='o', label='Without Evidence (Total)')\n",
    "plt.plot(avg_context_length, supports_accuracies_without_evidence, linestyle='dashed', marker='o', label='Without Evidence (Supports)')\n",
    "plt.plot(avg_context_length, refutes_accuracies_without_evidence, linestyle='dashed', marker='o', label='Without Evidence (Refutes)')\n",
    "\n",
    "\n",
    "plt.xlabel('Avg. Context Length')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracies w.r.t. Avg. Context Length')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(0.95, 0.7))\n",
    "plt.grid(alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e55818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "filenames = [\n",
    "    \"predictions-parametric-2way.csv\",\n",
    "    \"predictions-parametric-3way.csv\",\n",
    "    \"predictions-gold-fine-2way.csv\",\n",
    "    \"predictions-gold-fine-3way.csv\",\n",
    "    \"predictions-gold-coarse-2way.csv\",\n",
    "    \"predictions-gold-coarse-3way.csv\",\n",
    "]\n",
    "\n",
    "avg_context_length = [\n",
    "    \"Parametric 2-way\",\n",
    "    \"Parametric 3-way\",\n",
    "    \"Gold fine 2-way\",\n",
    "    \"Gold fine 3-way\",\n",
    "    \"Gold coarse 2-way\",\n",
    "    \"Gold fine 3-way\",\n",
    "]\n",
    "\n",
    "correct_list = []\n",
    "total_list = []\n",
    "supp_correct_list = []\n",
    "supp_total_list = []\n",
    "ref_correct_list = []\n",
    "ref_total_list = []\n",
    "nef_supp_list = []\n",
    "nef_ref_list = []\n",
    "\n",
    "for filename in filenames:\n",
    "    correct, supp_correct, supp_total, ref_correct, ref_total, nef_supp, nef_ref = eval_accuracy(filename)\n",
    "    correct_list.append(correct)\n",
    "    total_list.append(supp_total + ref_total)\n",
    "    supp_correct_list.append(supp_correct)\n",
    "    supp_total_list.append(supp_total)\n",
    "    ref_correct_list.append(ref_correct)\n",
    "    ref_total_list.append(ref_total)\n",
    "    nef_supp_list.append(nef_supp)\n",
    "    nef_ref_list.append(nef_ref)\n",
    "\n",
    "x = np.arange(len(avg_context_length))\n",
    "bar_width = 0.12\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "ax.bar(\n",
    "    x - 2.75 * bar_width,\n",
    "    total_list,\n",
    "    width=bar_width,\n",
    "    color=\"orange\",\n",
    "    edgecolor=\"orange\",\n",
    "    hatch=\"/\",\n",
    "    alpha=0.25,\n",
    "    label=\"No. of claims\",\n",
    ")\n",
    "ax.bar(\n",
    "    x - 2.75 * bar_width,\n",
    "    correct_list,\n",
    "    width=bar_width,\n",
    "    color=\"orange\",\n",
    "    label=\"No. of correct predictions\",\n",
    ")\n",
    "\n",
    "\n",
    "ax.bar(\n",
    "    x - 1 * bar_width,\n",
    "    supp_total_list,\n",
    "    width=bar_width,\n",
    "    color=\"green\",\n",
    "    edgecolor=\"green\",\n",
    "    hatch=\"/\",\n",
    "    alpha=0.25,\n",
    "    label=\"No. of claims with label 'SUPPORTS'\",\n",
    ")\n",
    "ax.bar(\n",
    "    x - 1 * bar_width,\n",
    "    supp_correct_list,\n",
    "    width=bar_width,\n",
    "    color=\"green\",\n",
    "    label=\"No. of correct predictions for label 'SUPPORTS'\",\n",
    ")\n",
    "ax.bar(\n",
    "    x - 0 * bar_width,\n",
    "    nef_supp_list,\n",
    "    width=bar_width,\n",
    "    color=\"grey\",\n",
    "    hatch=\"o\",\n",
    "    alpha=0.5,\n",
    "    label=\"No. of claims with label 'SUPPORTS' but predicted 'NOT ENOUGH EVIDENCE'\",\n",
    ")\n",
    "\n",
    "ax.bar(\n",
    "    x + 1.75 * bar_width,\n",
    "    ref_total_list,\n",
    "    width=bar_width,\n",
    "    color=\"red\",\n",
    "    edgecolor=\"red\",\n",
    "    hatch=\"/\",\n",
    "    alpha=0.25,\n",
    "    label=\"No. of claims with label 'REFUTES'\",\n",
    ")\n",
    "ax.bar(\n",
    "    x + 1.75 * bar_width,\n",
    "    ref_correct_list,\n",
    "    width=bar_width,\n",
    "    color=\"red\",\n",
    "    label=\"No. of correct predictions for label 'REFUTES'\",\n",
    ")\n",
    "ax.bar(\n",
    "    x + 2.75 * bar_width,\n",
    "    nef_ref_list,\n",
    "    width=bar_width,\n",
    "    color=\"grey\",\n",
    "    hatch=\"O\",\n",
    "    alpha=0.5,\n",
    "    label=\"No. of claims with label 'REFUTES' but predicted 'NOT ENOUGH EVIDENCE'\",\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"No. of Claims\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(avg_context_length)\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c90eb",
   "metadata": {},
   "source": [
    "## Chain of Thought (CoT) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa52720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CoT_prompt_template = \"\"\"\n",
    "You are a helpful, smart, kind, and efficient AI assistant who always fulfills the user requests to the best of its abilities and strictly sticks to the given instructions.\n",
    "\n",
    "Instructions:\n",
    "You answer SUPPORTS if context EXPLICITLY supports the claim.\n",
    "You answer REFUTES if the context EXPLICITLY refutes the claim.\n",
    "You answer NOT ENOUGH EVIDENCE if the context does not provide enough information to explicitly support or refute the claim.\n",
    "\n",
    "Context:\n",
    "Barack Hussein Obama II[a] (born August 4, 1961) is an American politician who served as the 44th president of the United States from 2009 to 2017. As a member of the Democratic Party, he was the first African-American president in United States history. Obama previously served as a U.S. senator representing Illinois from 2005 to 2008, as an Illinois state senator from 1997 to 2004, and as a community service organizer, civil rights lawyer, and university lecturer. \n",
    "\n",
    "Claim:\n",
    "Obama served as a US senator before becoming president.\n",
    "\n",
    "Answer:\n",
    "The context states that Barack Obama served as United States senator representing Illinois from 2005 to 2008.\n",
    "It also mentions that he served as president from 2009 to 2017.\n",
    "Hence, the context SUPPORTS the claim.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Claim:\n",
    "{claim}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "predictions = pd.read_csv(\"predictions-gold-coarse-3way.csv\")\n",
    "\n",
    "total = len(predictions)\n",
    "counter = 0\n",
    "new_predictions = []\n",
    "for _, item in predictions.iterrows():\n",
    "    counter += 1\n",
    "    print(f\"Processing {counter}/{total}\", end=\"\\r\")\n",
    "\n",
    "    if item[\"label\"] != item[\"prediction\"]:\n",
    "        label = item[\"label\"]\n",
    "        prediction = item[\"prediction\"]\n",
    "        context = item[\"context\"]\n",
    "        claim = item[\"claim\"]\n",
    "\n",
    "        if label == \"REFUTES\" and label != prediction:\n",
    "            prompt = CoT_prompt_template.format(context=context, claim=claim)\n",
    "            response = generator(prompt, max_new_tokens=100)\n",
    "\n",
    "            new_prediction = None\n",
    "            if \"SUPPORTS\" in response:\n",
    "                new_prediction = \"SUPPORTS\"\n",
    "            elif \"REFUTES\" in response:\n",
    "                new_prediction = \"REFUTES\"\n",
    "            elif \"NOT ENOUGH EVIDENCE\" in response:\n",
    "                new_prediction = \"NOT ENOUGH EVIDENCE\"\n",
    "\n",
    "            new_predictions.append({\"claim\": claim, \"context\": context, \"label\": label, \"old_prediction\": prediction, \"new_prediction\": \"REFUTES\", \"response\": response})\n",
    "\n",
    "new_predictions_df = pd.DataFrame(new_predictions)\n",
    "new_predictions_df.to_csv(\"predictions-gold-coarse-3way-refutes-CoT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc84af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions_df = pd.read_csv(\"predictions-gold-coarse-3way-refutes-CoT.csv\")\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for _, item in new_predictions_df.iterrows():\n",
    "    total += 1\n",
    "    label = item[\"label\"]\n",
    "    prediction = item[\"new_prediction\"]\n",
    "    response = \"\".join(item[\"response\"].split(\"\\n\")[0:2])\n",
    "    if \"REFUTES\" in response:\n",
    "        print(response)\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Total Accuracy: {correct}/{total} ({correct/total:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
